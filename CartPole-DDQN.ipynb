{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "new_path = r'/usr/local/lib/python2.7/dist-packages'\n",
    "sys.path.append(new_path)\n",
    "\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[114]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "seed_val = 111\n",
    "np.random.seed(seed_val)\n",
    "random.seed(seed_val+1)\n",
    "tf.set_random_seed(seed_val+2)\n",
    "env.seed(seed_val+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential, clone_model\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "replay_buffer_length = 6000\n",
    "learning_rate = 0.002\n",
    "batch_size = 32\n",
    "N_ACTIONS = env.action_space.n\n",
    "eps_min = 0.01\n",
    "n_episodes = 3000\n",
    "# Neural net architecture, each layer must specify number of neurons and activation function\n",
    "neural_net_arch = [[16, 'relu'], [16, 'relu']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_schedule = np.append(np.linspace(1, eps_min, int(0.4*n_episodes)), eps_min * np.ones(n_episodes-int(0.4*n_episodes)))\n",
    "states = deque(maxlen= replay_buffer_length)\n",
    "actions = deque(maxlen= replay_buffer_length)\n",
    "rewards = deque(maxlen= replay_buffer_length)\n",
    "next_states = deque(maxlen= replay_buffer_length)\n",
    "done_flags = deque(maxlen= replay_buffer_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(target, prediction):\n",
    "    error = prediction - target\n",
    "    return K.mean(K.sqrt(1+K.square(error))-1, axis=-1)\n",
    "\n",
    "def preprocess(img):\n",
    "    return np.reshape(img, (1,4))\n",
    "\n",
    "def select_action(q_model, cur_state, eps):\n",
    "    if random.random() < eps:\n",
    "        return random.choice(range(N_ACTIONS))\n",
    "    else:\n",
    "        return np.argmax(q_model.predict(cur_state))\n",
    "\n",
    "def store_memory(states, actions, rewards, next_states, done_flags,\n",
    "                 state, action, reward, next_state, done):\n",
    "    states.append(state)\n",
    "    actions.append(action)\n",
    "    rewards.append(reward)\n",
    "    next_states.append(next_state)\n",
    "    done_flags.append(done)\n",
    "    \n",
    "# def q_learning_update(gamma, alpha, cur_state, reward, next_state, action, q_vals):\n",
    "#     target = reward + gamma * np.max(q_vals[next_state])\n",
    "#     q_vals[cur_state][action] = (1 - alpha) * q_vals[cur_state][action] + alpha * target\n",
    "    \n",
    "\n",
    "def replay(q_target_model, q_model, states, actions, rewards, next_states, done_flags):\n",
    "\n",
    "    minibatch_indices = random.sample(range(replay_buffer_length), batch_size)\n",
    "    states_minibatch = np.reshape(np.array(states)[minibatch_indices], (batch_size, 4))\n",
    "    actions_minibatch = np.array(actions)[minibatch_indices]\n",
    "    rewards_minibatch = np.array(rewards)[minibatch_indices]\n",
    "    next_states_minibatch = np.reshape(np.array(next_states)[minibatch_indices], (batch_size, 4))\n",
    "    done_flags_minibatch = np.array(done_flags)[minibatch_indices]\n",
    "\n",
    "#     from IPython.core.debugger import set_trace\n",
    "#     set_trace()\n",
    " \n",
    "    # y_minibatch currently gives one value for each state (corresponding to the Q_target for best action)\n",
    "    # but to train we need a shape of (batch_size, n_actions) for the loss\n",
    "    # setting the Loss[i] = 0 for all action_i except the best action\n",
    "\n",
    "    # Two options: init y_minibatch with zeros or q_model values\n",
    "    # zeros will move the q_model to a higher loss corresponding to actions which were not taken\n",
    "    # q_model values will have zero loss for actions which were not taken\n",
    "    y_minibatch = q_model.predict(states_minibatch)\n",
    "    # y_minibatch = np.zeros((batch_size, N_ACTIONS))\n",
    "\n",
    "    # q_val_non_target =  #shape: (32,2)\n",
    "    # q_val_target =  #shape: (32,2)\n",
    "    \n",
    "    # Two options: reward for completion: -1 vs 1\n",
    "    # Hypothesis: gamma = 1 will work better for ending reward=1 because it can see 'longer' in the future \n",
    "    # that it's not going to get much reward\n",
    "    # Generally reward = -10 training faster to solve the game\n",
    "    # y_minibatch[range(batch_size), actions_minibatch] = rewards_minibatch + (1. - done_flags_minibatch) * (1 * q_val_target[range(batch_size),best_action_non_target])\n",
    "    best_action_non_target = np.argmax(q_model.predict(next_states_minibatch), axis=1) #shape: (32,)\n",
    "    y_minibatch[range(batch_size), actions_minibatch] = rewards_minibatch + (1. - done_flags_minibatch) * (gamma * q_target_model.predict(next_states_minibatch)[range(batch_size),best_action_non_target] )\n",
    "    \n",
    "    q_model.fit(x=states_minibatch, y=y_minibatch, verbose=0)\n",
    "\n",
    "def reset_q_target(q_model):\n",
    "    model_copy = clone_model(q_model)\n",
    "    model_copy.set_weights(q_model.get_weights())\n",
    "    return model_copy\n",
    "\n",
    "def create_q_model():\n",
    "    model = Sequential()\n",
    "    if (len(neural_net_arch) >= 1):\n",
    "        model.add(Dense(neural_net_arch[0][0], activation=neural_net_arch[0][1], input_dim=env.observation_space.shape[0]))\n",
    "    if (len(neural_net_arch) >= 2):\n",
    "        model.add(Dense(neural_net_arch[1][0], activation=neural_net_arch[1][1]))\n",
    "    if (len(neural_net_arch) >= 3):\n",
    "        model.add(Dense(neural_net_arch[2][0], activation=neural_net_arch[2][1]))\n",
    "                  \n",
    "    model.add(Dense(N_ACTIONS, activation='linear'))\n",
    "    opt = optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "    # model.compile(optimizer=opt, loss=huber_loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "q_model = create_q_model()\n",
    "q_target_model = reset_q_target(q_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_steps = 0\n",
    "ret=[]\n",
    "q_vals=[]\n",
    "eps_vals=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_state = preprocess(env.reset())\n",
    "for i in range(replay_buffer_length):\n",
    "    action = select_action(q_model, cur_state, 1) #Full exploration for replay buffer initiation\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    next_state = preprocess(next_state)\n",
    "    store_memory(states, actions, rewards, next_states, done_flags,\n",
    "                     cur_state, action, reward, next_state, done)\n",
    "    if done:\n",
    "        cur_state = preprocess(env.reset())\n",
    "    cur_state = next_state\n",
    "\n",
    "for episode in range(1,n_episodes+1):\n",
    "    # preprocess state\n",
    "    cur_state = preprocess(env.reset())\n",
    "    \n",
    "    for t in range(300):\n",
    "        env.render()\n",
    "        global_steps += 1\n",
    "\n",
    "        if global_steps % 100 == 0:\n",
    "            q_target_model = reset_q_target(q_model)\n",
    "\n",
    "        action = select_action(q_model, cur_state, eps_schedule[episode])\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        reward = -10 if done else reward\n",
    "        next_state = preprocess(next_state)\n",
    "        \n",
    "        if done:\n",
    "            ret.append(t)\n",
    "            q_vals.append(q_model.predict(cur_state)[0,action])\n",
    "            eps_vals.append(eps_schedule[episode])\n",
    "            # next_state = np.zeros(cur_state.shape)\n",
    "            store_memory(states, actions, rewards, next_states, done_flags,\n",
    "                         cur_state, action, reward, next_state, done)\n",
    "            \n",
    "            if (t > 180):\n",
    "                print(\"Reward:\", t)\n",
    "            if (episode % 10 == 0):\n",
    "                print(\"Episode:\", episode)\n",
    "                print(\"Avg Reward (Last 10):\", np.mean(ret[-10:-1]))\n",
    "                if episode % 100 == 0:\n",
    "                    print(\"Avg Reward (Last 100):\", np.mean(ret[-100:-1]))\n",
    "            break\n",
    "        \n",
    "        store_memory(states, actions, rewards, next_states, done_flags,\n",
    "                     cur_state, action, reward, next_state, done)\n",
    "        \n",
    "        cur_state = next_state\n",
    "        \n",
    "        replay(q_target_model, q_model, states, actions, rewards, next_states, done_flags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
