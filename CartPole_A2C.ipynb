{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPole-A2C.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "TdsJUgPF5kS7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "531792ff-ce53-4270-9f47-a5b406a8cff0"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "import gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/50/ed4a03d2be47ffd043be2ee514f329ce45d98a30fe2d1b9c61dea5a9d861/gym-0.10.5.tar.gz (1.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.5)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Collecting pyglet>=1.2.0 (from gym)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 10.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.4.16)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Running setup.py bdist_wheel for gym ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/cb/14/71/f4ab006b1e6ff75c2b54985c2f98d0644fffe9c1dddc670925\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, gym\n",
            "Successfully installed gym-0.10.5 pyglet-1.3.2\n",
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nKBTKsD15kTB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c380b53e-072b-4324-faa4-6ee914f13acc"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "# tf.enable_eager_execution()\n",
        "\n",
        "seed_val = 111\n",
        "np.random.seed(seed_val)\n",
        "random.seed(seed_val+1)\n",
        "tf.set_random_seed(seed_val+2)\n",
        "env.seed(seed_val+3)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[114]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "W6duDePE5kTI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bcbc1472-8e93-4a14-b9e7-019f265b6e6f"
      },
      "cell_type": "code",
      "source": [
        "import multiprocessing as mp\n",
        "import os\n",
        "os.system(\"taskset -p 0xff %d\" % os.getpid())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "Lwncr5ka5kTM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess(img):\n",
        "    return np.reshape(img, (1,4))\n",
        "\n",
        "def discounted_rewards(rewards, dones, gamma):\n",
        "    discounted = []\n",
        "    r = 0\n",
        "    for reward, done in zip(rewards[::-1],dones[::-1]): #revert the rewards to start discounting from behind\n",
        "        r = reward + gamma*r*(1.-done) #return = (reward) if done else (reward + gamma*return_tplus1)\n",
        "        discounted.append(r)\n",
        "    return discounted[::-1] #revert the discounts to normal order"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sNDmv4GX5kTP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def worker(remote, env_fun):\n",
        "    env = env_fun\n",
        "    while True:\n",
        "        cmd, data = remote.recv()\n",
        "        if cmd == 'get_spaces':\n",
        "            remote.send((env.action_space, env.observation_space))\n",
        "        elif cmd == 'step':\n",
        "            ob, rew, done, info = env.step(data)\n",
        "            if done:\n",
        "                ob = env.reset()\n",
        "            remote.send((ob, rew, done))\n",
        "        elif cmd == 'reset':\n",
        "            ob = env.reset()\n",
        "            remote.send(ob)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "class SubProcVecEnv():\n",
        "    \"\"\"\n",
        "    :param: env_funcs - list of agent environment functions\n",
        "    \"\"\"\n",
        "    def __init__(self, env_funcs):\n",
        "        self.nenvs = len(env_funcs)\n",
        "        self.remotes, self.work_remotes = zip(*[mp.Pipe() for _ in range(self.nenvs)])\n",
        "        self.ps = [mp.Process(target=worker, args=(work_remote, env_fn))\n",
        "                   for (work_remote, env_fn) in zip(self.work_remotes, env_funcs)]\n",
        "        for p in self.ps:\n",
        "            p.start()\n",
        "            \n",
        "        self.remotes[0].send(('get_spaces', None)) # Ask about the env space details\n",
        "        self.action_space, self.observation_space = self.remotes[0].recv()\n",
        "        \n",
        "    def step(self, actions):\n",
        "        assert len(actions) == len(self.remotes)\n",
        "        for remote, action in zip(self.remotes, actions):\n",
        "            remote.send(('step', action))\n",
        "            \n",
        "        results = [remote.recv() for remote in self.remotes]\n",
        "        obs, rewards, dones = zip(*results)\n",
        "        return np.stack(obs), np.stack(rewards), np.stack(dones)\n",
        "    \n",
        "    def reset(self):\n",
        "        for remote in self.remotes:\n",
        "            remote.send(('reset', None))\n",
        "            \n",
        "        obs = [remote.recv() for remote in self.remotes]\n",
        "        \n",
        "        return np.stack(obs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o-fXC2hR5kTS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class A2C():\n",
        "    def __init__(self, num_envs):\n",
        "        ## setup model\n",
        "        ## setup environment\n",
        "        self.nenv = num_envs\n",
        "        self.env = self.makeallenvironments(num_envs)\n",
        "        self.env.reset()\n",
        "    \n",
        "    @staticmethod\n",
        "    def makeallenvironments(num_envs=4):\n",
        "        return SubProcVecEnv([gym.make('CartPole-v0') for _ in range(num_envs)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1BKyyAOv5kTV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Policy():\n",
        "    def __init__(self, sess, obs_space, ac_space, nbatch, nsteps=1, reuse=False):\n",
        "        self.sess = sess\n",
        "        input_shape = (nbatch,) + obs_space.shape\n",
        "        num_actions = agent.env.action_space.n\n",
        "        \n",
        "        self.X_input = tf.placeholder(tf.float32, input_shape, name=\"Ob\") #input observation state\n",
        "        \n",
        "        with tf.variable_scope(\"model\", reuse=reuse):\n",
        "            h = self._build_model(self.X_input)\n",
        "            \n",
        "            self.policy_logits = tf.layers.dense(h, units=num_actions, name=\"policylogits\")\n",
        "            self.value_fn = tf.layers.dense(h, units=1, name=\"valuefn\")\n",
        "            action = tf.squeeze(tf.multinomial(logits=self.policy_logits, num_samples = 1))\n",
        "            \n",
        "        def select_action(obs):\n",
        "            a, vf = self.sess.run((action, self.value_fn), {self.X_input: obs})\n",
        "            return a, vf\n",
        "        \n",
        "        def value(obs):\n",
        "            v = self.sess.run(self.value_fn, {self.X_input: obs})\n",
        "            return v\n",
        "            \n",
        "        self.select_action = select_action\n",
        "        self.value = value\n",
        "    \n",
        "    def _build_model(self, X_input):\n",
        "        h1 = tf.layers.dense(X_input, units=100, activation=tf.nn.relu, name=\"layer1\")\n",
        "        h2 = tf.layers.dense(X_input, units=10, activation=tf.nn.relu, name=\"layer2\")\n",
        "        return h2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DUHN24qZ5kTZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Runner:\n",
        "    def __init__(self, env, policy_model, nsteps=5, gamma=0.99):\n",
        "        self.env = env\n",
        "        self.model = policy_model\n",
        "        self.gamma = gamma\n",
        "        nenvs = env.nenvs\n",
        "        self.batch_ob_shape = (nenvs * nsteps,) + env.observation_space.shape\n",
        "        self.nsteps = nsteps\n",
        "        self.initial_state = None\n",
        "        self.states = self.initial_state\n",
        "        # Init env variables\n",
        "        self.obs = np.zeros((nenvs,) + env.observation_space.shape, dtype=np.float32)\n",
        "        self.dones = [False for _ in range(nenvs)]\n",
        "    \n",
        "    def run(self):\n",
        "        mb_states = self.states\n",
        "        mb_obs, mb_actions, mb_rewards, mb_values, mb_dones = [], [], [], [], []\n",
        "        for n in range(self.nsteps):\n",
        "            # get the actions to take a step\n",
        "            actions, values = self.model.select_action(self.obs)\n",
        "            mb_obs.append(np.copy(self.obs)) #start states\n",
        "            mb_actions.append(actions) #actions\n",
        "            mb_values.append(values) #Value of the states\n",
        "            mb_dones.append(self.dones) #Done status of the episode before taking action\n",
        "            \n",
        "            # take a step\n",
        "            obs, rewards, dones = self.env.step(actions)\n",
        "            self.dones = dones\n",
        "            mb_rewards.append(rewards)\n",
        "            \n",
        "            # check which agents returned with done=True\n",
        "            for n, done in enumerate(dones):\n",
        "                if done:\n",
        "                    obs[n] = obs[n] * 0 # Reset obs to zeros if action led to completion of episode\n",
        "            self.obs = obs\n",
        "        \n",
        "        mb_dones.append(self.dones) #last done stores whether episode ended after taking the last action\n",
        "        mb_obs = np.asarray(mb_obs).swapaxes(1,0).reshape(self.batch_ob_shape) #first n rows are first obs of n agents and so on..\n",
        "        mb_rewards = np.asarray(mb_rewards).swapaxes(1,0)\n",
        "        mb_actions = np.asarray(mb_actions, dtype=np.uint8).swapaxes(1,0)\n",
        "        mb_dones = np.asarray(mb_dones, dtype=np.bool).swapaxes(1,0)\n",
        "        mb_values = np.asarray(mb_values).swapaxes(1,0)\n",
        "        mb_masks = mb_dones[:,:-1] # Stores done values of episode before the action is taken\n",
        "        mb_dones = mb_dones[:,1:] # Stores done values of episode after the action is taken\n",
        "        last_values = self.model.value(self.obs).tolist()\n",
        "        \n",
        "        # calculate returns for each agent\n",
        "        for n, (rewards, done, value) in enumerate(zip(mb_rewards, mb_dones, last_values)):\n",
        "            rewards = rewards.tolist() #helps in appending list later on\n",
        "            dones = done.tolist()\n",
        "            if dones[-1] == 0: #if last action led to end of episode, use last_value to estimate return\n",
        "                rewards = discounted_rewards(rewards + value, dones + [False], self.gamma)[:-1] #removing the return corresponding to value as it was just needed to calculate other returns\n",
        "            else: #rollout complete\n",
        "                rewards = discounted_rewards(rewards, dones, self.gamma)\n",
        "            mb_rewards[n] = rewards #mb_rewards now stores discounted returns rather than just rewards\n",
        "        \n",
        "        mb_actions = mb_actions.flatten()\n",
        "        mb_rewards = mb_rewards.flatten()\n",
        "        mb_values = mb_values.flatten()\n",
        "        mb_masks = mb_masks.flatten()\n",
        "        \n",
        "        return mb_obs, mb_actions, mb_rewards, mb_values, mb_masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vzfxLBI15kTc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "    def __init__(self, sess, obs_space, ac_space, nenvs=3, nsteps=5):\n",
        "        nbatch = nenvs * nsteps # batch size. nenvs: no. of agents, nsteps: no. of steps to be taken by an agent\n",
        "        A = tf.placeholder(tf.int32, [nbatch])# actions\n",
        "        ADV = tf.placeholder(tf.float32, [nbatch]) # advantage\n",
        "        R = tf.placeholder(tf.float32, [nbatch]) # returns\n",
        "#         LR = # learning rate\n",
        "        \n",
        "        step_model = Policy(sess, obs_space, ac_space, nenvs, nsteps=1, reuse=False)\n",
        "        train_model = Policy(sess, obs_space, ac_space, nbatch=nenvs*nsteps, nsteps=nsteps, reuse=True)\n",
        "        \n",
        "        # get loss\n",
        "#         A_onehot = tf.one_hot(A, depth=2)\n",
        "        neglogp_ac = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=train_model.policy_logits, labels=A)\n",
        "        pg_loss = tf.reduce_mean(ADV * neglogp_ac)\n",
        "        vf_loss = tf.reduce_mean(tf.squared_difference(tf.squeeze(train_model.value_fn), R))\n",
        "#         entropy = tf.reduce_mean(tf.softmax())\n",
        "        loss = pg_loss + vf_loss * 0.5\n",
        "        \n",
        "        # get grads\n",
        "        params = tf.trainable_variables(scope=\"model\")\n",
        "        grads = tf.gradients(loss, params)\n",
        "        grads, grad_norm = tf.clip_by_global_norm(grads, clip_norm=0.5)\n",
        "        grads_and_vars = list(zip(grads, params))\n",
        "        # apply grads\n",
        "        trainer = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
        "        _train = trainer.apply_gradients(grads_and_vars)\n",
        "        # update learning rate\n",
        "        \n",
        "        def train(obs, actions, returns, values, masks):\n",
        "            advs = returns - values\n",
        "            td_map = {A: actions, train_model.X_input: obs, ADV: advs, R: returns}\n",
        "            policy_loss, value_loss, _ = sess.run([pg_loss, vf_loss, _train], td_map)\n",
        "            return policy_loss, value_loss\n",
        "        \n",
        "        self.train = train\n",
        "        self.select_action = step_model.select_action #Give access to Policy class methods so that Runner can use them\n",
        "        self.value = step_model.value \n",
        "        self.step_model = step_model\n",
        "        self.train_model = train_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KiCBfLnH5kTg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Initialize Session and Classes"
      ]
    },
    {
      "metadata": {
        "id": "TN_ncTRa5kTh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ab14e8f8-da16-48fe-928b-be13dc25c5b1"
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph() \n",
        "sess = tf.Session()\n",
        "agent = A2C(num_envs=3)\n",
        "model = Model(sess, nenvs=agent.nenv, nsteps=5, obs_space=agent.env.observation_space, ac_space=agent.env.action_space)\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "runner = Runner(env=agent.env, policy_model=model, nsteps=5)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KJSfSMta5kTj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train"
      ]
    },
    {
      "metadata": {
        "id": "svYfV6fr5kTk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        },
        "outputId": "3cf02f37-8189-4efb-f9cc-2a30e8abce13"
      },
      "cell_type": "code",
      "source": [
        "for update in range(int(1e4)):\n",
        "    mb_obs, mb_actions, mb_returns, mb_values, mb_masks = runner.run()\n",
        "    policy_loss, value_loss = model.train(mb_obs, mb_actions, mb_returns, mb_values, mb_masks)\n",
        "    if update % 100 == 0:\n",
        "        print(update, policy_loss, value_loss, max(mb_returns))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 2.0814865 10.76047 4.846010781352588\n",
            "100 2.2015047 10.429535 4.952228981825924\n",
            "200 1.7732319 8.841066 5.269997582357253\n",
            "300 1.4925781 6.050584 5.746770421736947\n",
            "400 2.126329 10.914888 6.14216848078202\n",
            "500 1.7906512 8.629291 6.707398127504541\n",
            "600 1.3612577 7.3886385 6.654279523937549\n",
            "700 1.7631238 10.254335 7.343393514855129\n",
            "800 2.1942298 10.100694 8.160577580737689\n",
            "900 1.1893611 7.4110503 8.356927828183547\n",
            "1000 1.3835261 11.233779 9.903813710716552\n",
            "1100 1.7351397 9.980422 10.275102835718943\n",
            "1200 0.33484414 10.652118 10.392599864951679\n",
            "1300 1.7845998 8.589515 11.311508457208038\n",
            "1400 1.541416 8.521596 11.926182147846248\n",
            "1500 -0.15182273 12.427348 12.876987636517347\n",
            "1600 -1.0193843 19.525433 13.343023331652\n",
            "1700 1.9802113 8.640136 15.618260604939469\n",
            "1800 1.6983973 7.971852 16.247355012138456\n",
            "1900 1.6486589 7.529094 17.08967160133626\n",
            "2000 1.6100909 7.750879 18.184345515472668\n",
            "2100 1.6157478 7.650494 18.708125748415867\n",
            "2200 1.9313197 9.985378 19.84891457560318\n",
            "2300 -0.6559055 46.526306 20.289728414346193\n",
            "2400 1.6561427 7.85922 20.7778098802528\n",
            "2500 -0.38354665 42.73445 21.348542127054145\n",
            "2600 1.7030747 6.895694 22.19244857293137\n",
            "2700 -2.8756099 93.09252 22.785384397157383\n",
            "2800 -1.6078938 72.08349 23.78986183298163\n",
            "2900 1.7572924 7.417995 24.294545647076127\n",
            "3000 0.8521097 34.972034 25.293833602056328\n",
            "3100 1.0619701 36.800686 26.164128222459656\n",
            "3200 1.4730963 6.222061 26.904966971545377\n",
            "3300 -3.1930509 137.2783 27.654902276532155\n",
            "3400 -2.368594 113.27577 28.965869253851917\n",
            "3500 -4.1501794 162.0381 29.26029837656657\n",
            "3600 -4.2390394 165.30612 30.330751690314802\n",
            "3700 1.4386495 5.4339113 31.31194085471062\n",
            "3800 -5.692529 215.08893 31.88849199509733\n",
            "3900 -1.5048314 95.78128 33.45965856244806\n",
            "4000 -3.9136343 199.14452 32.6926366060978\n",
            "4100 1.4424571 5.8544436 34.63941384270502\n",
            "4200 1.3315762 5.2789583 35.60824692757982\n",
            "4300 1.3158287 4.4227724 35.92653387676833\n",
            "4400 -1.7168541 137.30492 36.62379259552932\n",
            "4500 1.1991028 4.102687 37.22125221046665\n",
            "4600 -1.676679 75.395805 37.84624273776013\n",
            "4700 1.3034706 3.238313 39.419934211554384\n",
            "4800 1.0679489 3.896588 40.01267413786668\n",
            "4900 1.1196512 4.2480993 41.1221165780542\n",
            "5000 1.1086345 4.052232 42.338308870263354\n",
            "5100 1.254563 3.591638 43.359390468145065\n",
            "5200 0.19070797 196.03563 44.06098796292106\n",
            "5300 -7.324587 494.8382 46.55841791017318\n",
            "5400 -4.3965487 238.90045 45.951914341552374\n",
            "5500 1.135483 3.5916488 46.970445638816564\n",
            "5600 1.151115 3.180412 47.88088119283708\n",
            "5700 1.0224863 2.9902387 48.94959500566624\n",
            "5800 0.84812164 2.626958 49.69472899789728\n",
            "5900 0.88940126 2.7968304 56.180368388082705\n",
            "6000 -5.155494 522.10065 51.5626192329706\n",
            "6100 0.82761586 2.558206 53.29505331618381\n",
            "6200 -12.294025 978.0109 53.3313438087082\n",
            "6300 0.82672006 2.0882292 55.90227316085937\n",
            "6400 0.8158998 2.1301873 57.51486125374477\n",
            "6500 -4.775845 393.23883 59.0037688360282\n",
            "6600 0.8387698 2.1049178 59.99360834441048\n",
            "6700 0.76606303 1.6518817 60.85716988096441\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6800 0.6477199 1.371029 61.60565953190271\n",
            "6900 0.752258 1.5933479 62.68654440955661\n",
            "7000 0.6369572 1.4801755 63.6614303771915\n",
            "7100 0.6401216 1.0358013 65.3443966094509\n",
            "7200 0.61413115 1.2463948 66.40852858774245\n",
            "7300 0.46533698 0.74049455 67.55147682665464\n",
            "7400 0.5215283 1.2279035 69.64236055910843\n",
            "7500 0.46475324 0.7742249 70.52580210616593\n",
            "7600 0.4065706 0.59356594 71.5878227402763\n",
            "7700 0.52943254 0.8862467 72.95894851580427\n",
            "7800 0.4139941 0.5564574 74.47543054331359\n",
            "7900 0.45541364 0.75134075 75.79546324075167\n",
            "8000 0.46109068 0.6137286 77.4727266670783\n",
            "8100 -9.184757 1505.3105 79.04075161233119\n",
            "8200 0.34591994 0.4018933 80.12651942687155\n",
            "8300 0.27118397 0.3255874 81.35517663077603\n",
            "8400 0.41612715 0.5381705 82.8743069078597\n",
            "8500 -13.64532 1223.5844 85.84861767641445\n",
            "8600 0.3028769 0.36651742 87.54741536229385\n",
            "8700 0.2713418 0.21657147 88.23724447128998\n",
            "8800 -2.5510657 1023.48175 89.29754399209595\n",
            "8900 0.09270546 0.05999767 91.89314212550799\n",
            "9000 0.111214854 0.048658468 93.20882153597451\n",
            "9100 0.07445013 0.04302911 95.63852159342697\n",
            "9200 -40.047215 3513.59 96.36733438725369\n",
            "9300 -0.75331604 620.8957 98.37038329957743\n",
            "9400 -0.12324453 0.05273436 98.76267975502206\n",
            "9500 -0.09763471 0.061059494 99.21821495920734\n",
            "9600 -0.070451126 0.023282086 99.29671197878338\n",
            "9700 -0.010872601 0.00742841 99.66259123778876\n",
            "9800 -0.020070408 0.011486903 99.43981903249606\n",
            "9900 -0.0028690079 0.0045530596 101.84404418945313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I4dPS8mV5kTo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# List of Trainable Variables in the Model"
      ]
    },
    {
      "metadata": {
        "id": "mFzxEMgc5kTp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "5cfea33e-b3ab-4380-88d9-181f8f40b7dd"
      },
      "cell_type": "code",
      "source": [
        "tf.trainable_variables()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'model/layer1/kernel:0' shape=(4, 100) dtype=float32_ref>,\n",
              " <tf.Variable 'model/layer1/bias:0' shape=(100,) dtype=float32_ref>,\n",
              " <tf.Variable 'model/layer2/kernel:0' shape=(4, 10) dtype=float32_ref>,\n",
              " <tf.Variable 'model/layer2/bias:0' shape=(10,) dtype=float32_ref>,\n",
              " <tf.Variable 'model/policylogits/kernel:0' shape=(10, 2) dtype=float32_ref>,\n",
              " <tf.Variable 'model/policylogits/bias:0' shape=(2,) dtype=float32_ref>,\n",
              " <tf.Variable 'model/valuefn/kernel:0' shape=(10, 1) dtype=float32_ref>,\n",
              " <tf.Variable 'model/valuefn/bias:0' shape=(1,) dtype=float32_ref>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "cPnJJF_n5kTx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Test"
      ]
    },
    {
      "metadata": {
        "id": "G3XO1Fhx5kTy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "7711a1da-1ba3-4dc3-e8b4-485f88f1be8c"
      },
      "cell_type": "code",
      "source": [
        "cur_state = agent.env.reset()\n",
        "rewards_run = []\n",
        "for i in range(20):\n",
        "    total=0\n",
        "    while True:\n",
        "        ac, val = model.select_action(cur_state)\n",
        "        next_state, rewards, dones = agent.env.step(ac)\n",
        "        total+=rewards[0]\n",
        "        cur_state = next_state\n",
        "        if dones[0]==True:\n",
        "            break\n",
        "    print(total)\n",
        "    rewards_run.append(total)\n",
        "np.mean(rewards_run)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "197.0\n",
            "200.0\n",
            "198.0\n",
            "181.0\n",
            "134.0\n",
            "200.0\n",
            "182.0\n",
            "200.0\n",
            "188.0\n",
            "175.0\n",
            "136.0\n",
            "197.0\n",
            "170.0\n",
            "200.0\n",
            "152.0\n",
            "200.0\n",
            "200.0\n",
            "122.0\n",
            "181.0\n",
            "191.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "180.2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "j92XiaaV7LK7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}