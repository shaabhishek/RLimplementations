{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "new_path = r'/usr/local/lib/python2.7/dist-packages'\n",
    "sys.path.append(new_path)\n",
    "\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[114]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "seed_val = 111\n",
    "np.random.seed(seed_val)\n",
    "random.seed(seed_val+1)\n",
    "tf.set_random_seed(seed_val+2)\n",
    "env.seed(seed_val+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "os.system(\"taskset -p 0xff %d\" % os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    return np.reshape(img, (1,4))\n",
    "\n",
    "def discounted_rewards(rewards, dones, gamma):\n",
    "    discounted = []\n",
    "    r = 0\n",
    "    for reward, done in zip(rewards[::-1],dones[::-1]): #revert the rewards to start discounting from behind\n",
    "        r = reward + gamma*r*(1.-done) #return = (reward) if done else (reward + gamma*return_tplus1)\n",
    "        discounted.append(r)\n",
    "    return discounted[::-1] #revert the discounts to normal order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(remote, env_fun):\n",
    "    env = env_fun\n",
    "    while True:\n",
    "        cmd, data = remote.recv()\n",
    "        if cmd == 'get_spaces':\n",
    "            remote.send((env.action_space, env.observation_space))\n",
    "        elif cmd == 'step':\n",
    "            ob, rew, done, info = env.step(data)\n",
    "            if done:\n",
    "                ob = env.reset()\n",
    "            remote.send((ob, rew, done))\n",
    "        elif cmd == 'reset':\n",
    "            ob = env.reset()\n",
    "            remote.send(ob)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "class SubProcVecEnv():\n",
    "    \"\"\"\n",
    "    :param: env_funcs - list of agent environment functions\n",
    "    \"\"\"\n",
    "    def __init__(self, env_funcs):\n",
    "        self.nenvs = len(env_funcs)\n",
    "        self.remotes, self.work_remotes = zip(*[mp.Pipe() for _ in range(self.nenvs)])\n",
    "        self.ps = [mp.Process(target=worker, args=(work_remote, env_fn))\n",
    "                   for (work_remote, env_fn) in zip(self.work_remotes, env_funcs)]\n",
    "        for p in self.ps:\n",
    "            p.start()\n",
    "            \n",
    "        self.remotes[0].send(('get_spaces', None)) # Ask about the env space details\n",
    "        self.action_space, self.observation_space = self.remotes[0].recv()\n",
    "        \n",
    "    def step(self, actions):\n",
    "        assert len(actions) == len(self.remotes)\n",
    "        for remote, action in zip(self.remotes, actions):\n",
    "            remote.send(('step', action))\n",
    "            \n",
    "        results = [remote.recv() for remote in self.remotes]\n",
    "        obs, rewards, dones = zip(*results)\n",
    "        return np.stack(obs), np.stack(rewards), np.stack(dones)\n",
    "    \n",
    "    def reset(self):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('reset', None))\n",
    "            \n",
    "        obs = [remote.recv() for remote in self.remotes]\n",
    "        \n",
    "        return np.stack(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C():\n",
    "    def __init__(self, num_envs):\n",
    "        ## setup model\n",
    "        ## setup environment\n",
    "        self.nenv = num_envs\n",
    "        self.env = self.makeallenvironments(num_envs)\n",
    "        self.env.reset()\n",
    "    \n",
    "    @staticmethod\n",
    "    def makeallenvironments(num_envs=4):\n",
    "        return SubProcVecEnv([gym.make('CartPole-v0') for _ in range(num_envs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    def __init__(self, sess, obs_space, ac_space, nbatch, nsteps=1, reuse=False):\n",
    "        self.sess = sess\n",
    "        input_shape = (nbatch,) + obs_space.shape\n",
    "        num_actions = agent.env.action_space.n\n",
    "        \n",
    "        self.X_input = tf.placeholder(tf.float32, input_shape, name=\"Ob\") #input observation state\n",
    "        \n",
    "        with tf.variable_scope(\"model\", reuse=reuse):\n",
    "            h = self._build_model(self.X_input)\n",
    "            \n",
    "            self.policy_logits = tf.layers.dense(h, units=num_actions, name=\"policylogits\")\n",
    "            self.value_fn = tf.layers.dense(h, units=1, name=\"valuefn\")\n",
    "            action = tf.squeeze(tf.multinomial(logits=self.policy_logits, num_samples = 1))\n",
    "            \n",
    "        def select_action(obs):\n",
    "            a, vf = self.sess.run((action, self.value_fn), {self.X_input: obs})\n",
    "            return a, vf\n",
    "        \n",
    "        def value(obs):\n",
    "            v = self.sess.run(self.value_fn, {self.X_input: obs})\n",
    "            return v\n",
    "            \n",
    "        self.select_action = select_action\n",
    "        self.value = value\n",
    "    \n",
    "    def _build_model(self, X_input):\n",
    "        h1 = tf.layers.dense(X_input, units=50, activation=tf.nn.relu, name=\"layer1\")\n",
    "        h2 = tf.layers.dense(X_input, units=50, activation=tf.nn.relu, name=\"layer2\")\n",
    "        return h2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner:\n",
    "    def __init__(self, env, policy_model, nsteps=5, gamma=0.99):\n",
    "        self.env = env\n",
    "        self.model = policy_model\n",
    "        self.gamma = gamma\n",
    "        nenvs = env.nenvs\n",
    "        self.batch_ob_shape = (nenvs * nsteps,) + env.observation_space.shape\n",
    "        self.nsteps = nsteps\n",
    "        self.initial_state = None\n",
    "        self.states = self.initial_state\n",
    "        # Init env variables\n",
    "        self.obs = np.zeros((nenvs,) + env.observation_space.shape, dtype=np.float32)\n",
    "        self.dones = [False for _ in range(nenvs)]\n",
    "    \n",
    "    def run(self):\n",
    "        mb_states = self.states\n",
    "        mb_obs, mb_actions, mb_rewards, mb_values, mb_dones = [], [], [], [], []\n",
    "        for n in range(self.nsteps):\n",
    "            # get the actions to take a step\n",
    "            actions, values = self.model.select_action(self.obs)\n",
    "            mb_obs.append(np.copy(self.obs)) #start states\n",
    "            mb_actions.append(actions) #actions\n",
    "            mb_values.append(values) #Value of the states\n",
    "            mb_dones.append(self.dones) #Done status of the episode before taking action\n",
    "            \n",
    "            # take a step\n",
    "            obs, rewards, dones = self.env.step(actions)\n",
    "            self.dones = dones\n",
    "            mb_rewards.append(rewards)\n",
    "            \n",
    "            # check which agents returned with done=True\n",
    "            for n, done in enumerate(dones):\n",
    "                if done:\n",
    "                    obs[n] = obs[n] * 0 # Reset obs to zeros if action led to completion of episode\n",
    "            self.obs = obs\n",
    "        \n",
    "        mb_dones.append(self.dones) #last done stores whether episode ended after taking the last action\n",
    "        mb_obs = np.asarray(mb_obs).swapaxes(1,0).reshape(self.batch_ob_shape) #first n rows are first obs of n agents and so on..\n",
    "        mb_rewards = np.asarray(mb_rewards).swapaxes(1,0)\n",
    "        mb_actions = np.asarray(mb_actions, dtype=np.uint8).swapaxes(1,0)\n",
    "        mb_dones = np.asarray(mb_dones, dtype=np.bool).swapaxes(1,0)\n",
    "        mb_values = np.asarray(mb_values).swapaxes(1,0)\n",
    "        mb_masks = mb_dones[:,:-1] # Stores done values of episode before the action is taken\n",
    "        mb_dones = mb_dones[:,1:] # Stores done values of episode after the action is taken\n",
    "        last_values = self.model.value(self.obs).tolist()\n",
    "        \n",
    "        # calculate returns for each agent\n",
    "        for n, (rewards, done, value) in enumerate(zip(mb_rewards, mb_dones, last_values)):\n",
    "            rewards = rewards.tolist() #helps in appending list later on\n",
    "            dones = done.tolist()\n",
    "            if dones[-1] == 0: #if last action led to end of episode, use last_value to estimate return\n",
    "                rewards = discounted_rewards(rewards + value, dones + [False], self.gamma)[:-1] #removing the return corresponding to value as it was just needed to calculate other returns\n",
    "            else: #rollout complete\n",
    "                rewards = discounted_rewards(rewards, dones, self.gamma)\n",
    "            mb_rewards[n] = rewards #mb_rewards now stores discounted returns rather than just rewards\n",
    "        \n",
    "        mb_actions = mb_actions.flatten()\n",
    "        mb_rewards = mb_rewards.flatten()\n",
    "        mb_values = mb_values.flatten()\n",
    "        mb_masks = mb_masks.flatten()\n",
    "        \n",
    "        return mb_obs, mb_actions, mb_rewards, mb_values, mb_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, sess, obs_space, ac_space, nenvs=3, nsteps=5, env=env):\n",
    "        nbatch = nenvs * nsteps # batch size. nenvs: no. of agents, nsteps: no. of steps to be taken by an agent\n",
    "        A = tf.placeholder(tf.int32, [nbatch])# actions\n",
    "        ADV = tf.placeholder(tf.float32, [nbatch]) # advantage\n",
    "        R = tf.placeholder(tf.float32, [nbatch]) # returns\n",
    "#         LR = # learning rate\n",
    "        \n",
    "        step_model = Policy(sess, obs_space, ac_space, nenvs, nsteps=1, reuse=False)\n",
    "        train_model = Policy(sess, obs_space, ac_space, nbatch=nenvs*nsteps, nsteps=nsteps, reuse=True)\n",
    "        \n",
    "        # get loss\n",
    "#         A_onehot = tf.one_hot(A, depth=2)\n",
    "        neglogp_ac = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=train_model.policy_logits, labels=A)\n",
    "        pg_loss = tf.reduce_mean(ADV * neglogp_ac)\n",
    "        vf_loss = tf.reduce_mean(tf.squared_difference(tf.squeeze(train_model.value_fn), R))\n",
    "#         entropy = tf.reduce_mean(tf.softmax())\n",
    "        loss = pg_loss + vf_loss * 0.5\n",
    "        \n",
    "        # get grads\n",
    "        params = tf.trainable_variables(scope=\"model\")\n",
    "        grads = tf.gradients(loss, params)\n",
    "        grads, grad_norm = tf.clip_by_global_norm(grads, clip_norm=0.5)\n",
    "        grads_and_vars = list(zip(grads, params))\n",
    "        # apply grads\n",
    "        trainer = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
    "        _train = trainer.apply_gradients(grads_and_vars)\n",
    "        # update learning rate\n",
    "        \n",
    "        def train(obs, actions, returns, values, masks):\n",
    "            advs = returns - values\n",
    "            td_map = {A: actions, train_model.X_input: obs, ADV: advs, R: returns}\n",
    "            policy_loss, value_loss, _ = sess.run([pg_loss, vf_loss, _train], td_map)\n",
    "            return policy_loss, value_loss\n",
    "        \n",
    "        def test():\n",
    "            cur_state = env.reset()\n",
    "            rewards_run = []\n",
    "            for i in range(20):\n",
    "                total=0\n",
    "                while True:\n",
    "                    ac, val = self.select_action(cur_state)\n",
    "                    next_state, rewards, dones = env.step(ac)\n",
    "                    total+=rewards[0]\n",
    "                    cur_state = next_state\n",
    "                    if dones[0]==True:\n",
    "                        break\n",
    "            #     print(total)\n",
    "                rewards_run.append(total)\n",
    "            return np.mean(rewards_run)\n",
    "        \n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.select_action = step_model.select_action #Give access to Policy class methods so that Runner can use them\n",
    "        self.value = step_model.value \n",
    "        self.step_model = step_model\n",
    "        self.train_model = train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Session and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "sess = tf.Session()\n",
    "agent = A2C(num_envs=4)\n",
    "model = Model(sess, nenvs=agent.nenv, nsteps=5, obs_space=agent.env.observation_space, ac_space=agent.env.action_space, env=agent.env)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "runner = Runner(env=agent.env, policy_model=model, nsteps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.0486693 10.851652 24.4\n",
      "100 2.019149 10.476603 19.45\n",
      "200 1.4260519 7.6669946 24.9\n",
      "300 1.964711 10.820755 23.45\n",
      "400 1.734512 14.58198 19.7\n",
      "500 1.8278265 9.542002 18.55\n",
      "600 1.4158782 12.327335 13.05\n",
      "700 -0.54426223 28.9071 20.05\n",
      "800 -0.21735148 36.524704 19.4\n",
      "900 1.7283449 7.9389467 23.9\n",
      "1000 1.7048286 7.861163 30.3\n",
      "1100 -0.49795842 41.931664 33.0\n",
      "1200 1.5832342 7.882852 47.85\n",
      "1300 1.4206033 7.0707293 30.55\n",
      "1400 1.3592732 5.885133 39.6\n",
      "1500 -0.5926124 66.804535 31.75\n",
      "1600 -2.1639009 139.0397 30.85\n",
      "1700 0.51164323 43.425964 34.45\n",
      "1800 -0.82179654 138.67229 36.35\n",
      "1900 -1.8122896 151.7062 28.25\n",
      "2000 1.3380375 4.597879 43.8\n",
      "2100 0.19510005 124.89845 57.35\n",
      "2200 1.3803068 5.2232943 40.2\n",
      "2300 1.004555 3.0246568 28.2\n",
      "2400 1.0422978 2.9466472 47.2\n",
      "2500 1.068466 2.8858752 43.95\n",
      "2600 0.9632722 2.6428998 46.35\n",
      "2700 0.6283408 1.5167764 42.1\n",
      "2800 0.78928244 1.9304583 40.75\n",
      "2900 0.71027195 2.1154985 62.3\n",
      "3000 0.55342114 1.0682518 50.0\n",
      "3100 0.501516 1.189613 66.4\n",
      "3200 0.43467325 0.5889653 76.25\n",
      "3300 0.36141875 0.50447464 56.45\n",
      "3400 0.27524015 0.27466714 39.8\n",
      "3500 0.2622958 0.2100223 69.85\n",
      "3600 0.07177655 0.058012027 48.55\n",
      "3700 -0.13564353 0.07935171 103.05\n",
      "3800 0.0021577463 0.053242445 160.65\n",
      "3900 0.06326019 0.039458673 121.95\n",
      "4000 -1.0396105 1039.5244 58.45\n",
      "4100 -12.305765 2841.7153 86.4\n",
      "4200 0.035146695 0.01671597 109.75\n",
      "4300 -0.07828867 0.017281178 43.75\n",
      "4400 -0.714725 1359.411 38.0\n",
      "4500 0.09179485 0.06788827 43.15\n",
      "4600 0.19721702 0.38171527 55.4\n",
      "4700 0.03244818 0.029094791 66.05\n",
      "4800 0.09126502 0.062202234 19.45\n",
      "4900 0.08849661 0.078011006 47.3\n",
      "5000 -0.0037760257 0.1285722 82.9\n",
      "5100 0.07600496 0.059613597 79.0\n",
      "5200 0.059538465 0.070041105 39.05\n",
      "5300 0.119278416 0.29636413 42.35\n",
      "5400 -35.0994 6753.354 53.6\n",
      "5500 0.102059305 0.27190182 34.6\n",
      "5600 -10.978434 3503.7656 24.1\n",
      "5700 -11.994791 5334.081 33.45\n",
      "5800 -28.759562 10341.841 16.1\n",
      "5900 -0.004065676 0.14543338 45.5\n",
      "6000 -7.798307 2089.0466 31.6\n",
      "6100 0.022164568 0.037337437 27.8\n",
      "6200 -24.434402 5310.568 42.35\n",
      "6300 0.05729871 0.050776254 27.0\n",
      "6400 0.051442146 0.08790532 25.4\n",
      "6500 -28.194082 5335.0347 51.95\n",
      "6600 0.00036152118 0.06313849 44.7\n",
      "6700 -13.459666 2266.5034 47.25\n",
      "6800 -15.0015135 2997.9836 27.45\n",
      "6900 0.09320414 0.055196345 34.85\n",
      "7000 0.07282666 0.035103768 69.8\n",
      "7100 0.012949142 0.039676793 44.75\n",
      "7200 -0.008790588 0.2595641 21.85\n",
      "7300 0.08903192 0.105573654 42.6\n",
      "7400 0.1388444 0.19967441 54.1\n",
      "7500 0.028407628 0.022319699 41.15\n",
      "7600 0.0686442 0.07422325 42.45\n",
      "7700 -12.402153 8753.697 31.0\n",
      "7800 -0.1030436 0.114605285 19.9\n",
      "7900 0.017523173 0.0158606 24.3\n",
      "8000 0.073351726 0.1195335 19.85\n",
      "8100 -13.72554 2497.7717 30.5\n",
      "8200 -5.9427843 3188.6448 15.5\n",
      "8300 -36.761112 7201.09 49.65\n",
      "8400 -24.309963 3198.512 49.45\n",
      "8500 0.09484773 0.07296227 41.95\n",
      "8600 -25.527308 6998.3726 46.1\n",
      "8700 -4.3915777 2081.8506 41.45\n",
      "8800 -0.052390087 0.029670238 35.05\n",
      "8900 0.025697712 0.008323314 36.3\n",
      "9000 -13.630341 4299.627 39.7\n",
      "9100 -7.125375 2493.8772 20.35\n",
      "9200 0.039272748 0.04529687 32.3\n",
      "9300 0.021394707 0.019333314 31.35\n",
      "9400 -13.933678 4655.6533 47.65\n",
      "9500 -8.804743 2673.8281 20.9\n",
      "9600 -0.9635164 1024.653 59.9\n",
      "9700 -0.190703 0.087004885 66.2\n",
      "9800 -0.11614038 0.061133634 80.1\n",
      "9900 -0.29121011 0.6788775 75.6\n"
     ]
    }
   ],
   "source": [
    "for update in range(1*int(1e4)):\n",
    "    mb_obs, mb_actions, mb_returns, mb_values, mb_masks = runner.run()\n",
    "    policy_loss, value_loss = model.train(mb_obs, mb_actions, mb_returns, mb_values, mb_masks)\n",
    "    if update % 100 == 0:\n",
    "        print(update, policy_loss, value_loss, model.test())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of Trainable Variables in the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'model/layer1/kernel:0' shape=(4, 50) dtype=float32_ref>,\n",
       " <tf.Variable 'model/layer1/bias:0' shape=(50,) dtype=float32_ref>,\n",
       " <tf.Variable 'model/layer2/kernel:0' shape=(4, 50) dtype=float32_ref>,\n",
       " <tf.Variable 'model/layer2/bias:0' shape=(50,) dtype=float32_ref>,\n",
       " <tf.Variable 'model/policylogits/kernel:0' shape=(50, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'model/policylogits/bias:0' shape=(2,) dtype=float32_ref>,\n",
       " <tf.Variable 'model/valuefn/kernel:0' shape=(50, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'model/valuefn/bias:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cur_state = agent.env.reset()\n",
    "rewards_run = []\n",
    "for i in range(20):\n",
    "    total=0\n",
    "    while True:\n",
    "        ac, val = model.select_action(cur_state)\n",
    "        next_state, rewards, dones = agent.env.step(ac)\n",
    "        total+=rewards[0]\n",
    "        cur_state = next_state\n",
    "        if dones[0]==True:\n",
    "            break\n",
    "#     print(total)\n",
    "    rewards_run.append(total)\n",
    "np.mean(rewards_run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
